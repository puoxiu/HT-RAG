

## bge模型微调
1、数据集
利用LLM构造出三元组：[问题，正例，反例]

其中正例通常选取原文文档，并且设置一些与原文接近的文本；返例则是回答相反的答案，或者不相关的文档


## 消融实验




## 文档内容加载缓慢
url数量较大有400个左右，在运行之前要进行访问access校验，因此很耗时
解决：使用多线程（协程？傻傻分不清，还不太清楚python的并发实际怎么实现的，反正让大模型生成且有效就是 以后慢慢了解这些非核心）


## 显存爆炸
在实验过程中，我的url数量有400个。导致分割后的documents数据量也巨大，进而占用很大内存和cpu
解决方案效仿训练过程，设置批处理，每次加载batch_size个documents存向faiss



## embedding模型相似度问题

使用 [BAAI/bge-large-zh-v1.5] 模型，的效果很差：
比如,对于两个句子：
1. sentence1 = 是的，后面我们会加入rpm中，目前编译部署后可以使用lgraph_peer"
2. sentence2 = "对不起，我不太明白你的意思。"
计算它们的余弦相似度有： 0.6209

解决：对embedding模型微调
